{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/clairvoyance_logo.png\">\n",
    "\n",
    "# Clairvoyance: Time-series prediction\n",
    "\n",
    "## ML-AIM (http://vanderschaar-lab.com/)\n",
    "\n",
    "This notebook describes the user-guide of a time-series predictions application using Clairvoyance framework. Time-series prediction is defined as following: utilize both static and temporal features to predict certain labels in the future. For instance, using the temporal data (vitals, lab tests) and static data (demographic information), we predict 'whether the patient will die at the end of hospital stay' or 'whether the patient will get ventilator after 4 hours'. \n",
    "- One-shot prediction: Predict the patient state at the end of the time-series at certain time point.\n",
    "  - Example: Predict patient mortality (at the end of the hospital stays) after 24 hours from the admission.\n",
    "- Rolling window (online) prediction:\n",
    "  - Example: Predict ventilator after 24 hours from the current time point.\n",
    " \n",
    "<img src=\"figure/time-series-prediction-definition.png\">\n",
    "\n",
    "To run this tutorial, you need:\n",
    "### Temporal and static datasets for training and testing\n",
    "\n",
    "If users come with their own temporal and static datasets for training and testing, the users should save those files as 'data_name_temporal_train_data_eav.csv.gz', 'data_name_static_train_data.csv.gz', 'data_name_temporal_test_data_eav.csv.gz', 'data_name_static_test_data.csv.gz' in '../datasets/data/data_name/' directory.\n",
    "\n",
    "\n",
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/time-series-automl.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series prediction pipeline summary\n",
    "\n",
    "<img src=\"figure/time-series-prediction-block-diagram.png\">\n",
    "\n",
    "### Step 1: Load dataset\n",
    "  - Extract csv files from the original raw datasets in ../datasets/data/data_name/ directory.  \n",
    "  \n",
    "### Step 2: Preprocess dataset\n",
    "  - Preprocessing the raw data using various filters such as (1) replacing negative values to NaN, (2) do one-hot encidng for certain features, (3) do normalization.  \n",
    "  \n",
    "### Step 3: Define problem\n",
    "  - Set the time-series prediction problem that we want to solve. Set the problem (whether it is one-shot or online prediction), set the label, set the maximum sequence length, and set the treatment features. We also define the metric for evaluation and the task itself (whether classification or regression).\n",
    "\n",
    "### Step 4: Impute dataset\n",
    "  - Impute missing values in the preprocessed static and temporal datasets and return complete datasets.\n",
    "  \n",
    "### Step 5: Feature selection\n",
    "  - Select the relevant static and temporal features to the labels. You can skip the feature selection (set feature selection method = None).\n",
    "  \n",
    "### Step 6: Time-series model fit and predict\n",
    "  - After finishing the data preparation, we define the predictive models and train the model using the training dataset. After training, we use the trained model to predict the labels of the testing dataset.\n",
    "  \n",
    "### Step 7: Estimate uncertainty\n",
    "  - Estimate uncertainty of the predictor models and returns the uncertainty of the predictions.\n",
    "\n",
    "### Step 8: Interpret predictions\n",
    "  - Interpret the trained predictor model and return the instance-wise feature and temporal importance.\n",
    "\n",
    "### Step 9: Visualize results\n",
    "  - Visualize the various results such as performance, predictions, uncertainties, and interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import necessary packages\n",
    "\n",
    "Import necessary packages for the entire tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import PipelineComposer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load dataset\n",
    "\n",
    "Extract temporal and static datasets from 'data_name_temporal_train_data_eav.csv.gz', 'data_name_static_train_data.csv.gz', 'data_name_temporal_test_data_eav.csv.gz', 'data_name_static_test_data.csv.gz' in '../datasets/data/data_name/' directory.\n",
    "\n",
    "- CSVLoader: Load csv files from the original raw datasets in ../datasets/data/data_name/ directory.\n",
    "- file_names: mimic in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish data loading.\n"
     ]
    }
   ],
   "source": [
    "from datasets import CSVLoader\n",
    "\n",
    "# Define data name\n",
    "data_name = 'mimic'\n",
    "# Define data dictionary\n",
    "data_directory = '../datasets/data/'+data_name + '/' + data_name + '_'\n",
    "\n",
    "# Load train and test datasets\n",
    "data_loader_training = CSVLoader(static_file=data_directory + 'static_train_data.csv.gz',\n",
    "                                 temporal_file=data_directory + 'temporal_train_data_eav.csv.gz')\n",
    "\n",
    "data_loader_testing = CSVLoader(static_file=data_directory + 'static_test_data.csv.gz',\n",
    "                                temporal_file=data_directory + 'temporal_test_data_eav.csv.gz')\n",
    "\n",
    "dataset_training = data_loader_training.load()\n",
    "dataset_testing = data_loader_testing.load()\n",
    "\n",
    "print('Finish data loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess dataset\n",
    "\n",
    "Preprocess the raw data using multiple filters. In this tutorial, we replace all the negative values to NaN (using NegativeFilter), do one-hot encoding on 'admission_type' feature (using OneHotEncoder), and do MinMax Normalization (using Normalization). Preprocessing is done for both training and testing datasets. \n",
    "  - NegativeFilter: Replace negative values to NaN\n",
    "  - OneHotEncoder: One hot encoding certain features\n",
    "    - one_hot_encoding: input features that need to be one-hot encoded\n",
    "  - Normalization (3 options): MinMax, Standard, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preprocessing.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import FilterNegative, OneHotEncoder, Normalizer\n",
    "\n",
    "# (1) filter out negative values\n",
    "negative_filter = FilterNegative()\n",
    "# (2) one-hot encode categorical features\n",
    "one_hot_encoding = 'admission_type'\n",
    "onehot_encoder = OneHotEncoder(one_hot_encoding_features=[one_hot_encoding])\n",
    "# (3) Normalize features: 3 options (minmax, standard, none)\n",
    "normalization = 'minmax'\n",
    "normalizer = Normalizer(normalization)\n",
    "\n",
    "# Data preprocessing\n",
    "filter_pipeline = PipelineComposer(negative_filter, onehot_encoder, normalizer)\n",
    "\n",
    "dataset_training = filter_pipeline.fit_transform(dataset_training)\n",
    "dataset_testing = filter_pipeline.transform(dataset_testing)\n",
    "\n",
    "print('Finish preprocessing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define problem   \n",
    "\n",
    "Set the time-series prediction problem that we want to solve. Set the problem (whether it is one-shot or online prediction), set the label, set the maximum sequence length, and set the treatment features. We also define the metric for evaluation and the task itself (whether classification or regression). In this tutorial, we predict whether the patients will get ventilator after 4 hours (online setting).\n",
    "  - problem: 'one-shot'(one time prediction) or 'online'(rolling window prediction)\n",
    "    - 'one-shot': one time prediction at the end of the time-series \n",
    "    - 'online': preditcion at every time stamps of the time-series\n",
    "  - max_seq_len: maximum sequence length of time-series sequence\n",
    "  - label_name: the column name for the label(s)\n",
    "  - treatment: the column name for treatments\n",
    "  - window: x-hour ahead prediction.\n",
    "  \n",
    "  - other parameters:\n",
    "    - metric_name: auc, apr, mse, mae\n",
    "    - task: classification or regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18490/18490 [00:50<00:00, 362.93it/s]\n",
      "100%|██████████| 18490/18490 [00:48<00:00, 383.22it/s]\n",
      "100%|██████████| 18490/18490 [00:52<00:00, 352.41it/s]\n",
      "100%|██████████| 4610/4610 [00:09<00:00, 492.79it/s]\n",
      "100%|██████████| 4610/4610 [00:08<00:00, 527.31it/s]\n",
      "100%|██████████| 4610/4610 [00:09<00:00, 483.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish defining problem.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import ProblemMaker\n",
    "\n",
    "# Define parameters\n",
    "problem = 'online'\n",
    "max_seq_len = 48\n",
    "label_name = 'ventilator'\n",
    "treatment = None\n",
    "window = 12\n",
    "\n",
    "# Define problem \n",
    "problem_maker = ProblemMaker(problem=problem, label=[label_name],\n",
    "                             max_seq_len=max_seq_len, treatment=treatment, window = window)\n",
    "\n",
    "dataset_training = problem_maker.fit_transform(dataset_training)\n",
    "dataset_testing = problem_maker.fit_transform(dataset_testing)\n",
    "\n",
    "# Set other parameters\n",
    "metric_name = 'auc'\n",
    "task = 'classification'\n",
    "\n",
    "metric_sets = [metric_name, 'apr']\n",
    "metric_parameters =  {'problem': problem, 'label_name': [label_name]}\n",
    "\n",
    "print('Finish defining problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Impute dataset\n",
    "\n",
    "Impute missing values in the preprocessed static and temporal datasets and return complete datasets.\n",
    "  - Static imputation (6 options): mean, median, mice, missforest, knn, gain\n",
    "  - Temporal imputation (8 options): mean, median, linear, quadratic, cubic, spline, mrnn, tgain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish imputation.\n"
     ]
    }
   ],
   "source": [
    "import imputation\n",
    "\n",
    "# Set imputation models\n",
    "static_imputation_model = 'median'\n",
    "temporal_imputation_model = 'median'\n",
    "\n",
    "# Impute the missing data\n",
    "static_imputation = imputation.Imputation(imputation_model_name = static_imputation_model, data_type = 'static')\n",
    "temporal_imputation = imputation.Imputation(imputation_model_name = temporal_imputation_model, data_type = 'temporal')\n",
    "\n",
    "imputation_pipeline = PipelineComposer(static_imputation, temporal_imputation)\n",
    "\n",
    "dataset_training = imputation_pipeline.fit_transform(dataset_training)\n",
    "dataset_testing = imputation_pipeline.transform(dataset_testing)\n",
    "\n",
    "print('Finish imputation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Time-series model fit and predict\n",
    "\n",
    "After finishing the data preparation, we define the predictive models (6 options, RNN, GRU, LSTM, Attention, Temporal CNN, and Transformer), and train the model using the training dataset. We set validation set as the 20% of the training set for early stopping and best model saving. After training, we use the trained model to predict the labels of the testing dataset.\n",
    "\n",
    "- predictor_parameters:\n",
    "  - model_name: rnn, gru, lstm, attention, tcn, transformer\n",
    "  - model_parameters: network parameters such as numer of layers\n",
    "    - h_dim: hidden dimensions\n",
    "    - n_layer: layer number\n",
    "    - n_head: head number (only for transformer model)\n",
    "    - batch_size: number of samples in mini-batch\n",
    "    - epochs: number of epochs\n",
    "    - learning_rate: learning rate\n",
    "  - static_mode: how to utilize static features (concatenate or None)\n",
    "  - time_mode: how to utilize time information (concatenate or None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prediction' from '../prediction/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prediction.boosting' from '../prediction/boosting.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prediction.boosting\n",
    "importlib.reload(prediction.boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish predictor model training and testing.\n"
     ]
    }
   ],
   "source": [
    "# Set predictive model\n",
    "model_name = 'gbm'\n",
    "\n",
    "# Set model parameters\n",
    "model_parameters = {'n_estimators': 500,\n",
    "                    'max_depth': 4,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'static_mode': 'Concatenate',\n",
    "                    'time_mode': 'Concatenate',\n",
    "                    'verbose': True}\n",
    "\n",
    "# Set up validation for early stopping and best model saving\n",
    "dataset_training.train_val_test_split(prob_val=0.2, prob_test = 0.0)\n",
    "\n",
    "# Train the predictive model\n",
    "\n",
    "pred_class = prediction.boosting.GBM(task=task)\n",
    "pred_class.set_params(**model_parameters)\n",
    "    \n",
    "    \n",
    "pred_class.fit(dataset_training)\n",
    "# Return the predictions on the testing set\n",
    "test_y_hat = pred_class.predict(dataset_testing)\n",
    "\n",
    "print('Finish predictor model training and testing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "def evaluate(y, y_hat):\n",
    "    idx = np.where(y >= 0)[0]\n",
    "    y = y[idx]\n",
    "    y_hat = y_hat[idx] \n",
    "\n",
    "    y = ~(y == 0)\n",
    "    \n",
    "    y = y.flatten()\n",
    "    y_hat = y_hat.flatten()\n",
    "    \n",
    "    result = roc_auc_score(y, y_hat)\n",
    "    result2 = average_precision_score(y, y_hat)\n",
    "    return result, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5752072141433271, 0.5621684139087026)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dataset_testing.label, test_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4610, 48, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_testing.label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
